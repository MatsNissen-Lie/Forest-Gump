{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tester nyny</h3>\n",
    "<p><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unique_filename(base_name, folder=\"submissions\", move_up=True):\n",
    "    end = \"\"\n",
    "    if folder == \"models\":\n",
    "        end = \".pkl\"\n",
    "\n",
    "    # Use '..' to move up one folder level if move_up is True\n",
    "    parent_folder = \"..\" if move_up else \"\"\n",
    "\n",
    "    file_path = Path(parent_folder) / folder / f\"{base_name}.csv{end}\"\n",
    "    count = 1\n",
    "    while file_path.exists():\n",
    "        file_path = Path(parent_folder) / folder / \\\n",
    "            f\"{base_name}_{count}.csv{end}\"\n",
    "        count += 1\n",
    "    return str(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/matsalexander/Desktop/Forest Gump/final_submission/mats_explore_to_merge\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current working directory:\", current_dir)\n",
    "\n",
    "\n",
    "PATH = \"/Users/matsalexander/Desktop/Forest Gump/\"\n",
    "# Estimate\n",
    "X_train_estimated_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + 'A/X_train_estimated.parquet')\n",
    "X_train_estimated_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_train_estimated.parquet\")\n",
    "X_train_estimated_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_train_estimated.parquet\")\n",
    "\n",
    "# Test estimates\n",
    "X_test_estimated_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/X_test_estimated.parquet\")\n",
    "X_test_estimated_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_test_estimated.parquet\")\n",
    "X_test_estimated_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_test_estimated.parquet\")\n",
    "\n",
    "# Observations\n",
    "X_train_observed_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/X_train_observed.parquet\")\n",
    "X_train_observed_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_train_observed.parquet\")\n",
    "X_train_observed_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_train_observed.parquet\")\n",
    "\n",
    "# Targets\n",
    "Y_train_observed_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/train_targets.parquet\")\n",
    "Y_train_observed_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/train_targets.parquet\")\n",
    "Y_train_observed_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/train_targets.parquet\")\n",
    "\n",
    "test_df_example = pd.read_csv(PATH + \"test.csv\")\n",
    "\n",
    "best_submission: pd.DataFrame = pd.read_csv(\n",
    "    PATH + \"mikael/submissions/fourth_submission.csv\")\n",
    "\n",
    "optins = {\n",
    "    \"randomize\": False,\n",
    "    \"consecutive_threshold\": 6,\n",
    "    \"normalize\": False,\n",
    "    \"group_by_hour\": True,\n",
    "    \"unzip_date_feature\": True,\n",
    "}\n",
    "\n",
    "# make a options class with the options as attributes\n",
    "\n",
    "\n",
    "class Options:\n",
    "    randomize = False\n",
    "    consecutive_threshold = 6\n",
    "    normalize = False\n",
    "    group_by_hour = True\n",
    "    unzip_date_feature = True\n",
    "\n",
    "    def __init__(self, randomize=False, consecutive_threshold=6, normalize=False, group_by_hour=True, unzip_date_feature=True) -> None:\n",
    "        self.randomize = randomize\n",
    "        self.consecutive_threshold = consecutive_threshold\n",
    "        self.normalize = normalize\n",
    "        self.group_by_hour = group_by_hour\n",
    "        self.unzip_date_feature = unzip_date_feature\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_combined_data(self, test_data=False):\n",
    "        locations = [\"A\", \"B\", \"C\"]\n",
    "        dfs = []\n",
    "        for index, location in enumerate(locations):\n",
    "            if test_data:\n",
    "                dfs.append(self.get_test_data(location))\n",
    "            else:\n",
    "                dfs.append(self.get_data(location))\n",
    "\n",
    "            dfs[index] = self.onehot_location(dfs[index], location)\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "        if test_data:\n",
    "            return df\n",
    "        return df[[c for c in df if c not in ['pv_measurement']] +  # pv measurement is the target and is at the end columns\n",
    "                  ['pv_measurement']]\n",
    "\n",
    "    def get_data(self, location: str, keeptime=False) -> pd.DataFrame:\n",
    "        train, targets = self.get_training_data_by_location(location)\n",
    "        return self.handle_data(train, targets, keeptime=keeptime)\n",
    "\n",
    "    def get_test_data(self, location: str) -> pd.DataFrame:\n",
    "        test_data = self.get_test_data_by_location(location)\n",
    "        return self.handle_data(test_data)\n",
    "\n",
    "    def handle_data(self, df, targets=pd.DataFrame(), keeptime=False):\n",
    "        df[\"date_calc\"] = pd.to_datetime(df[\"date_calc\"])\n",
    "        df[\"date_forecast\"] = pd.to_datetime(df[\"date_forecast\"])\n",
    "\n",
    "        # df = self.add_time_since_calucation(df)\n",
    "        df = self.onehot_estimated(df)\n",
    "        df = self.unzip_date_feature(df)\n",
    "        df = self.grouped_by_hour(df)\n",
    "\n",
    "        df[\"time\"] = df[\"date_forecast\"]\n",
    "        df.drop([\"date_forecast\"], axis=1, inplace=True)\n",
    "\n",
    "        if not targets.empty:\n",
    "            df = self.merge_train_target(df, targets)\n",
    "\n",
    "        df.drop(columns=[\"time\"], axis=1, inplace=True)\n",
    "\n",
    "        df = self.absolute_values(df)\n",
    "        return df\n",
    "\n",
    "    # –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– helper funciton ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "    def get_training_data_by_location(self, location):\n",
    "        if location == \"A\":\n",
    "            X_train_observed_x = X_train_observed_a\n",
    "            X_train_estimated_x = X_train_estimated_a\n",
    "            Y_train_x = Y_train_observed_a\n",
    "        elif location == \"B\":\n",
    "            X_train_observed_x = X_train_observed_b\n",
    "            X_train_estimated_x = X_train_estimated_b\n",
    "            Y_train_x = Y_train_observed_b\n",
    "        elif location == \"C\":\n",
    "            X_train_observed_x = X_train_observed_c\n",
    "            X_train_estimated_x = X_train_estimated_c\n",
    "            Y_train_x = Y_train_observed_c\n",
    "        else:\n",
    "            raise Exception(\"location must be A, B or C\")\n",
    "        train = pd.concat(\n",
    "            [X_train_observed_x, X_train_estimated_x]).reset_index(drop=True)\n",
    "        return train, Y_train_x\n",
    "\n",
    "    def get_test_data_by_location(self, location: str,  normalize=False) -> pd.DataFrame:\n",
    "        if location == \"A\":\n",
    "            df = X_test_estimated_a\n",
    "        elif location == \"B\":\n",
    "            df = X_test_estimated_b\n",
    "        elif location == \"C\":\n",
    "            df = X_test_estimated_c\n",
    "        else:\n",
    "            raise Exception(\"location must be A, B or C\")\n",
    "        return df.copy()\n",
    "\n",
    "    def unzip_date_feature(self, df: pd.DataFrame, date_column: str = \"date_forecast\"):\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df[\"day_of_year\"] = df[\"date_forecast\"].dt.day_of_year\n",
    "        df[\"hour\"] = df[\"date_forecast\"].dt.hour\n",
    "        # df[\"month\"] = df[\"date_forecast\"].dt.month\n",
    "        return df\n",
    "\n",
    "    def add_time_since_calucation(self, df):  # denne er ikke så dum.\n",
    "        df[\"date_calc\"] = pd.to_datetime(df[\"date_calc\"])\n",
    "        df[\"calculated_ago\"] = (\n",
    "            df[\"date_forecast\"] - df[\"date_calc\"]).dt.total_seconds()\n",
    "        df[\"calculated_ago\"] = df[\"calculated_ago\"].fillna(\n",
    "            0) / 60/30\n",
    "        return df\n",
    "\n",
    "    def onehot_estimated(self, df):\n",
    "        df[\"estimated\"] = 0  # Initialize both columns to 0\n",
    "        df[\"observed\"] = 0\n",
    "        estimated_mask = df[\"date_calc\"].notna()\n",
    "        df.loc[estimated_mask, \"estimated\"] = 1\n",
    "        df.loc[~estimated_mask, \"observed\"] = 1\n",
    "        return df\n",
    "\n",
    "    def onehot_location(self, df, location):\n",
    "        if location == \"A\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 1, 0, 0\n",
    "        elif location == \"B\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 0, 1, 0\n",
    "        elif location == \"C\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 0, 0, 1\n",
    "        return df\n",
    "\n",
    "    def grouped_by_hour(self, df: pd.DataFrame, date_column: str = \"date_forecast\"):\n",
    "        df = df.groupby(pd.Grouper(key=date_column, freq=\"1H\")\n",
    "                        ).mean(numeric_only=True)\n",
    "        all_nan_mask = df.isnull().all(axis=1)\n",
    "        df = df[~all_nan_mask]\n",
    "        return df.reset_index()\n",
    "\n",
    "    def merge_train_target(self, x, y):\n",
    "        # henning får med alle pv measurments selv om han merger på inner time. Fordi resample fyller nan rows for alle timer som ikke er i datasettet.\n",
    "        merged = pd.merge(x, y, on=\"time\", how=\"right\")\n",
    "        mask = merged[\"pv_measurement\"].notna()\n",
    "        merged = merged.loc[mask].reset_index(drop=True)\n",
    "        return merged\n",
    "\n",
    "    def absolute_values(self, df: pd.DataFrame):\n",
    "        columns = list(df.columns)\n",
    "        df[columns] = df[columns].abs()\n",
    "        df = df.replace(-0.0, 0.0)\n",
    "        return df\n",
    "\n",
    "    def lag_features_by_1_hour(df, columns_to_lag):\n",
    "        lag_columns = [c for c in df.columns if \"_1h:\" in c]\n",
    "        df[lag_columns] = df[lag_columns].shift(1)\n",
    "        return df\n",
    "\n",
    "    def remove_consecutive_measurments_new(self, df: pd.DataFrame, consecutive_threshold=3, consecutive_threshold_zero=12, return_removed=False):\n",
    "        if consecutive_threshold < 2:\n",
    "            return df\n",
    "\n",
    "        column_to_check = 'pv_measurement'\n",
    "\n",
    "        mask = (df[column_to_check] != df[column_to_check].shift(1)).cumsum()\n",
    "        df['consecutive_group'] = df.groupby(\n",
    "            mask).transform('count')[column_to_check]\n",
    "\n",
    "        df[\"is_first_in_consecutive_group\"] = False\n",
    "        df['is_first_in_consecutive_group'] = df['consecutive_group'] != df['consecutive_group'].shift(\n",
    "            1)\n",
    "\n",
    "        # masks to remove rows\n",
    "        mask_non_zero = (df['consecutive_group'] >= consecutive_threshold) & (\n",
    "            df[\"pv_measurement\"] > 0) & (df[\"is_first_in_consecutive_group\"] == False)  # or df[\"direct_rad:W\"] == 0)\n",
    "\n",
    "        mask_zero = (df['consecutive_group'] >= consecutive_threshold_zero) & (\n",
    "            df[\"pv_measurement\"] == 0) & (df[\"is_first_in_consecutive_group\"] == False)\n",
    "\n",
    "        mask = mask_non_zero | mask_zero\n",
    "\n",
    "        if return_removed:\n",
    "            return df[mask]\n",
    "\n",
    "        df = df.loc[~mask]\n",
    "\n",
    "        df = df.drop(columns=[\"consecutive_group\",\n",
    "                     \"is_first_in_consecutive_group\"])\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def remove_consecutive_measurments_new_new(self, df: pd.DataFrame, consecutive_threshold=3, consecutive_threshold_zero=12, consecutive_threshold_zero_no_rad=20, return_removed=False):\n",
    "        if consecutive_threshold < 2:\n",
    "            return df\n",
    "\n",
    "        column_to_check = 'pv_measurement'\n",
    "\n",
    "        mask = (df[column_to_check] != df[column_to_check].shift(1)).cumsum()\n",
    "        df['consecutive_group'] = df.groupby(\n",
    "            mask).transform('count')[column_to_check]\n",
    "\n",
    "        df[\"is_first_in_consecutive_group\"] = False\n",
    "        df['is_first_in_consecutive_group'] = df['consecutive_group'] != df['consecutive_group'].shift(\n",
    "            1)\n",
    "\n",
    "        # masks to remove rows\n",
    "        mask_non_zero = (df['consecutive_group'] >= consecutive_threshold) & (\n",
    "            df[\"pv_measurement\"] > 0) & (df[\"is_first_in_consecutive_group\"] == False)  # or df[\"direct_rad:W\"] == 0)\n",
    "\n",
    "        tol = 10\n",
    "        mask_zero = (df['consecutive_group'] >= consecutive_threshold_zero) & (\n",
    "            df[\"pv_measurement\"] == 0) & (df[\"direct_rad:W\"] > tol)\n",
    "\n",
    "        mask_zero_no_rad = (df['consecutive_group'] >= consecutive_threshold_zero_no_rad) & (\n",
    "            df[\"pv_measurement\"] == 0) & (df[\"direct_rad:W\"] < tol)\n",
    "        mask = mask_non_zero | mask_zero | mask_zero_no_rad\n",
    "\n",
    "        if return_removed:\n",
    "            return df[mask]\n",
    "\n",
    "        df = df.loc[~mask]\n",
    "\n",
    "        df = df.drop(columns=[\"consecutive_group\",\n",
    "                     \"is_first_in_consecutive_group\"])\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def compare_mae(self, df: pd.DataFrame):\n",
    "        best_submission: pd.DataFrame = pd.read_csv(\n",
    "            PATH+\"mats/submissions/gluon_3_remove_consecutive_measurements_66.csv\")\n",
    "        best_submission = best_submission[[\"prediction\"]]\n",
    "\n",
    "        if best_submission.shape != df.shape:\n",
    "            print(\"best_submission\", best_submission.shape)\n",
    "            print(\"df\", df.shape)\n",
    "            raise Exception(\"Dataframe shape must be the same\")\n",
    "\n",
    "        return mean_absolute_error(\n",
    "            best_submission[\"prediction\"], df[\"prediction\"])\n",
    "\n",
    "    def split_train_tune(self, df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        df_estimated = df.loc[df[\"estimated\"] == 1]\n",
    "        df_observed = df.loc[df[\"estimated\"] == 0]\n",
    "\n",
    "        num_rows = len(df_estimated)\n",
    "        middle_index = num_rows // 2\n",
    "\n",
    "        df_estimated.sample(frac=1, random_state=42)\n",
    "        train_estimated = df_estimated.iloc[:middle_index]\n",
    "        tune = df_estimated.iloc[middle_index:]\n",
    "\n",
    "        train = pd.concat([df_observed, train_estimated])\n",
    "        return train, tune\n",
    "\n",
    "    def drop_columns(self, df: pd.DataFrame):\n",
    "        drop = [\n",
    "            # wind speed vector u, available up to 20000 m, from 1000 hPa to 10 hPa and on flight levels FL10-FL900[m/s] does not make sens at surfece level\n",
    "            \"wind_speed_w_1000hPa:ms\",\n",
    "            \"wind_speed_u_10m:ms\",  # same as above\n",
    "            \"wind_speed_v_10m:ms\",  # same as above\n",
    "            # \"snow_drift:idx\",\n",
    "            # \"snow_density:kgm3\",\n",
    "            # \"snow_melt_10min:mm\",  # veldig få verdier\n",
    "        ]\n",
    "        shared_columns = list(set(df.columns) & set(drop))\n",
    "        df = df.drop(columns=shared_columns)\n",
    "        return df\n",
    "\n",
    "    def drop_columns_new(self, df: pd.DataFrame):\n",
    "        drop = [\n",
    "            # wind speed vector u, available up to 20000 m, from 1000 hPa to 10 hPa and on flight levels FL10-FL900[m/s] does not make sens at surfece level\n",
    "            \"wind_speed_w_1000hPa:ms\",\n",
    "            \"wind_speed_u_10m:ms\",  # same as above\n",
    "            \"wind_speed_v_10m:ms\",  # same as above\n",
    "            \"snow_drift:idx\",\n",
    "            \"snow_density:kgm3\",\n",
    "            # \"snow_melt_10min:mm\",  # veldig få verdier\n",
    "        ]\n",
    "        shared_columns = list(set(df.columns) & set(drop))\n",
    "        df = df.drop(columns=shared_columns)\n",
    "        return df\n",
    "\n",
    "    def find_min_max_date_in_test(self) -> list:\n",
    "        locations = [\"A\", \"B\", \"C\"]\n",
    "        dates = []\n",
    "        for loc in locations:\n",
    "            df = self.get_test_data_by_location(loc)\n",
    "            df[\"date_forecast\"] = pd.to_datetime(df[\"date_forecast\"])\n",
    "            dates.append((df[\"date_forecast\"].min(),\n",
    "                         df[\"date_forecast\"].max()))\n",
    "        return dates\n",
    "\n",
    "    def split_train_summer_2021(self, df: pd.DataFrame):\n",
    "        dates = self.find_min_max_date_in_test()\n",
    "        # set the dates to the summer of 2021\n",
    "        dates = [(date[0].replace(year=2021), date[1].replace(year=2021))\n",
    "                 for date in dates]\n",
    "\n",
    "        summer2021 = df[(df[\"date_forecast\"] >= dates[0][0]) & (\n",
    "            df[\"date_forecast\"] <= dates[0][1])]\n",
    "\n",
    "        train = df[~df.index.isin(summer2021.index)]\n",
    "        return train, summer2021\n",
    "\n",
    "    def post_processing(self, df: pd.DataFrame, prediction_column: str = \"prediction_label\"):\n",
    "        df = df[[prediction_column]].rename(\n",
    "            columns={prediction_column: \"prediction\"}).reset_index(drop=True).rename_axis(index=\"id\")\n",
    "\n",
    "        df[\"prediction\"] = df[\"prediction\"].clip(lower=0)\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsalexander/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/matsalexander/Desktop/Forest Gump/final_submission/mats_explore_to_merge\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pipeline import Pipeline\n",
    "pipin = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_0 = pipin.get_data(\"A\")\n",
    "df2_0 = pipin.get_data(\"B\")\n",
    "df3_0 = pipin.get_data(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 3\n",
    "threshold2 = 12\n",
    "threshold3 = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_consecutive_measurments\n",
    "# 6/6 er henning sin beste\n",
    "df1_0 = pipin.remove_consecutive_measurments_new_new(df1_0, 3, 12, 21) #tipper oslo\n",
    "df2_0 = pipin.remove_consecutive_measurments_new_new(df2_0, 3, 12, 24) #tipper Trondheim\n",
    "df3_0 = pipin.remove_consecutive_measurments_new_new(df3_0, 3, 12, 24) #tipper Trondheim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33850, 47) (25667, 47) (20905, 47)\n"
     ]
    }
   ],
   "source": [
    "df1_0 = pipin.drop_columns(df1_0)\n",
    "df2_0 = pipin.drop_columns(df2_0)\n",
    "df3_0 = pipin.drop_columns(df3_0)\n",
    "print(df1_0.shape, df2_0.shape, df3_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>diffuse_rad_1h:J</th>\n",
       "      <th>...</th>\n",
       "      <th>super_cooled_liquid_water:kgm2</th>\n",
       "      <th>t_1000hPa:K</th>\n",
       "      <th>total_cloud_cover:p</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>estimated</th>\n",
       "      <th>observed</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>hour</th>\n",
       "      <th>pv_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.700</td>\n",
       "      <td>1.22825</td>\n",
       "      <td>1728.949951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1728.949951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.299988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.225006</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>40386.476562</td>\n",
       "      <td>3.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.700</td>\n",
       "      <td>1.22350</td>\n",
       "      <td>1689.824951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1689.824951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.299988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.899994</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33770.648438</td>\n",
       "      <td>3.350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.875</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1563.224976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1563.224976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.649994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.950012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13595.500000</td>\n",
       "      <td>3.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.425</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1283.425049</td>\n",
       "      <td>208.649994</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1283.425049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.674988</td>\n",
       "      <td>0.300</td>\n",
       "      <td>526.775024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2321.850098</td>\n",
       "      <td>2.725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.950</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>32468.150391</td>\n",
       "      <td>23.10</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>11.975</td>\n",
       "      <td>22068.949219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.450012</td>\n",
       "      <td>99.224998</td>\n",
       "      <td>11634.799805</td>\n",
       "      <td>2.550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_humidity_2m:gm3  air_density_2m:kgm3  ceiling_height_agl:m  \\\n",
       "0                     7.700              1.22825           1728.949951   \n",
       "1                     7.700              1.22350           1689.824951   \n",
       "2                     7.875              1.21975           1563.224976   \n",
       "3                     8.425              1.21800           1283.425049   \n",
       "4                     8.950              1.21800           1003.500000   \n",
       "\n",
       "   clear_sky_energy_1h:J  clear_sky_rad:W  cloud_base_agl:m  dew_or_rime:idx  \\\n",
       "0               0.000000             0.00       1728.949951              0.0   \n",
       "1               0.000000             0.00       1689.824951              0.0   \n",
       "2               0.000000             0.00       1563.224976              0.0   \n",
       "3             208.649994             0.75       1283.425049              0.0   \n",
       "4           32468.150391            23.10       1003.500000              0.0   \n",
       "\n",
       "   dew_point_2m:K  diffuse_rad:W  diffuse_rad_1h:J  ...  \\\n",
       "0      280.299988          0.000          0.000000  ...   \n",
       "1      280.299988          0.000          0.000000  ...   \n",
       "2      280.649994          0.000          0.000000  ...   \n",
       "3      281.674988          0.300        526.775024  ...   \n",
       "4      282.500000         11.975      22068.949219  ...   \n",
       "\n",
       "   super_cooled_liquid_water:kgm2  t_1000hPa:K  total_cloud_cover:p  \\\n",
       "0                             0.0   286.225006           100.000000   \n",
       "1                             0.0   286.899994           100.000000   \n",
       "2                             0.0   286.950012           100.000000   \n",
       "3                             0.0   286.750000           100.000000   \n",
       "4                             0.0   286.450012            99.224998   \n",
       "\n",
       "   visibility:m  wind_speed_10m:ms  estimated  observed  day_of_year  hour  \\\n",
       "0  40386.476562              3.600        0.0       1.0        153.0  22.0   \n",
       "1  33770.648438              3.350        0.0       1.0        153.0  23.0   \n",
       "2  13595.500000              3.050        0.0       1.0        154.0   0.0   \n",
       "3   2321.850098              2.725        0.0       1.0        154.0   1.0   \n",
       "4  11634.799805              2.550        0.0       1.0        154.0   2.0   \n",
       "\n",
       "   pv_measurement  \n",
       "0            0.00  \n",
       "1            0.00  \n",
       "2            0.00  \n",
       "3            0.00  \n",
       "4           19.36  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = TabularDataset(df1_0)\n",
    "train2 = TabularDataset(df2_0)\n",
    "train3 = TabularDataset(df3_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231110_180924/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231110_180924/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   777.44 GB / 994.66 GB (78.2%)\n",
      "Train Data Rows:    33850\n",
      "Train Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 634.97254, 1168.75397)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19090.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.77 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 45 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.2s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.43 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-266.17\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-267.0048\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsalexander/Desktop/Forest Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictor1 \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpv_measurement\u001b[39;49m\u001b[39m\"\u001b[39;49m, eval_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean_absolute_error\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train1,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# time_limit=60*60*1.5,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# hyperparameters='extrme', \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     presets\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbest_quality\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# tuning_data = tuning1,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# use_bag_holdout=True,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# num_bag_folds= 6,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# refit_full = True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# auto_stack = True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# num_bag_sets= 10,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# set_best_to_refit_full= True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# num_stack_levels = 2,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# verbosity = 3\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsalexander/Desktop/Forest%20Gump/final_submission/mats_explore_to_merge/gluon_3_consecutive_best.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    987\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m    988\u001b[0m     X_val\u001b[39m=\u001b[39;49mtuning_data,\n\u001b[1;32m    989\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[1;32m    990\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[1;32m    991\u001b[0m     num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds,\n\u001b[1;32m    992\u001b[0m     num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[1;32m    993\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    994\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    995\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[1;32m    996\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    997\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m    998\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    999\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m   1000\u001b[0m     verbosity\u001b[39m=\u001b[39;49mverbosity,\n\u001b[1;32m   1001\u001b[0m     use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout,\n\u001b[1;32m   1002\u001b[0m )\n\u001b[1;32m   1003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[1;32m   1006\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1007\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[1;32m   1013\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m--> 157\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    158\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    159\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    160\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    161\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    162\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    163\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[1;32m    164\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[1;32m    165\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    166\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    167\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    168\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[1;32m    171\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/tabular/trainer/auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m log_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m20\u001b[39m, log_str)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(\n\u001b[1;32m    115\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    116\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    117\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    118\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    119\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    120\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    121\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    122\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m    123\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[1;32m    124\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    125\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    126\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    127\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2371\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[1;32m   2370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m-> 2371\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_multi_levels(\n\u001b[1;32m   2372\u001b[0m     X,\n\u001b[1;32m   2373\u001b[0m     y,\n\u001b[1;32m   2374\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m   2375\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m   2376\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m   2377\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m   2378\u001b[0m     level_start\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2379\u001b[0m     level_end\u001b[39m=\u001b[39;49mnum_stack_levels \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2380\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m   2381\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2382\u001b[0m )\n\u001b[1;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:395\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    393\u001b[0m         core_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_core)\n\u001b[1;32m    394\u001b[0m         aux_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 395\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[1;32m    396\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    397\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    398\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    399\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    400\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    401\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    402\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    403\u001b[0m         base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[1;32m    404\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level,\n\u001b[1;32m    405\u001b[0m         aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level,\n\u001b[1;32m    406\u001b[0m         name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[1;32m    407\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    408\u001b[0m         infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:539\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    537\u001b[0m     core_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m    538\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m--> 539\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level_core(\n\u001b[1;32m    540\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    541\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    542\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    543\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    544\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    545\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m    546\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    547\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    548\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    549\u001b[0m     base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[1;32m    550\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcore_kwargs,\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(\n\u001b[1;32m    555\u001b[0m         X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs\n\u001b[1;32m    556\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:673\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[1;32m    672\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi(\n\u001b[1;32m    674\u001b[0m     X\u001b[39m=\u001b[39;49mX_init,\n\u001b[1;32m    675\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    676\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    677\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    678\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    679\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m    680\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    681\u001b[0m     stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[1;32m    682\u001b[0m     compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[1;32m    683\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m    684\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    685\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2321\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2320\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m-> 2321\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_initial(\n\u001b[1;32m   2322\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   2323\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   2324\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m   2325\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m   2326\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats_initial,\n\u001b[1;32m   2327\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[1;32m   2328\u001b[0m         feature_prune_kwargs\u001b[39m=\u001b[39;49mfeature_prune_kwargs,\n\u001b[1;32m   2329\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m   2330\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2331\u001b[0m     )\n\u001b[1;32m   2332\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[1;32m   2333\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2170\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2169\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 2170\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_fold(\n\u001b[1;32m   2171\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m   2172\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[1;32m   2173\u001b[0m         k_fold_start\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2174\u001b[0m         k_fold_end\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m   2175\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats,\n\u001b[1;32m   2176\u001b[0m         n_repeat_start\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2177\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m   2178\u001b[0m         time_split\u001b[39m=\u001b[39;49mtime_split,\n\u001b[1;32m   2179\u001b[0m         time_ratio\u001b[39m=\u001b[39;49mtime_ratio,\n\u001b[1;32m   2180\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args,\n\u001b[1;32m   2181\u001b[0m     )\n\u001b[1;32m   2183\u001b[0m multi_fold_time_elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m multi_fold_time_start\n\u001b[1;32m   2184\u001b[0m \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2278\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2277\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[0;32m-> 2278\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_full(\n\u001b[1;32m   2279\u001b[0m     X, y, model, time_limit\u001b[39m=\u001b[39;49mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   2280\u001b[0m )\n\u001b[1;32m   2282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m   2283\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[1;32m   2048\u001b[0m             k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start\n\u001b[1;32m   2049\u001b[0m         )\n\u001b[1;32m   2050\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[0;32m-> 2051\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_and_save(\n\u001b[1;32m   2052\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   2053\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   2054\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   2055\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m   2056\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m   2057\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m   2058\u001b[0m         stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[1;32m   2059\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   2060\u001b[0m         compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[1;32m   2061\u001b[0m         total_resources\u001b[39m=\u001b[39;49mtotal_resources,\n\u001b[1;32m   2062\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs,\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[1;32m   2064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m   2065\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1733\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[1;32m   1732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[1;32m   1735\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1684\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[1;32m   1680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1684\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[1;32m   1685\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 829\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py:169\u001b[0m, in \u001b[0;36mStackerEnsembleModel._fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     time_limit \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n\u001b[0;32m--> 169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, time_limit\u001b[39m=\u001b[39;49mtime_limit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:266\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[39m# Reserve time for final refit model\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m folds_to_fit \u001b[39m/\u001b[39m (folds_to_fit \u001b[39m+\u001b[39m \u001b[39m1.2\u001b[39m)\n\u001b[0;32m--> 266\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_folds(\n\u001b[1;32m    267\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    268\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    269\u001b[0m     model_base\u001b[39m=\u001b[39;49mmodel_base,\n\u001b[1;32m    270\u001b[0m     X_pseudo\u001b[39m=\u001b[39;49mX_pseudo,\n\u001b[1;32m    271\u001b[0m     y_pseudo\u001b[39m=\u001b[39;49my_pseudo,\n\u001b[1;32m    272\u001b[0m     k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m    273\u001b[0m     k_fold_start\u001b[39m=\u001b[39;49mk_fold_start,\n\u001b[1;32m    274\u001b[0m     k_fold_end\u001b[39m=\u001b[39;49mk_fold_end,\n\u001b[1;32m    275\u001b[0m     n_repeats\u001b[39m=\u001b[39;49mn_repeats,\n\u001b[1;32m    276\u001b[0m     n_repeat_start\u001b[39m=\u001b[39;49mn_repeat_start,\n\u001b[1;32m    277\u001b[0m     save_folds\u001b[39m=\u001b[39;49msave_bag_folds,\n\u001b[1;32m    278\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    279\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[39m# FIXME: Cleanup self\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m refit_folds:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:592\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit_folds\u001b[0;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[39mfor\u001b[39;00m fold_fit_args \u001b[39min\u001b[39;00m fold_fit_args_list:\n\u001b[1;32m    591\u001b[0m     fold_fitting_strategy\u001b[39m.\u001b[39mschedule_fold_model_fit(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfold_fit_args)\n\u001b[0;32m--> 592\u001b[0m fold_fitting_strategy\u001b[39m.\u001b[39;49mafter_all_folds_scheduled()\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# No need to add child times or save child here as this already occurred in the fold_fitting_strategy\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_child(model\u001b[39m=\u001b[39mmodel, add_child_times\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py:508\u001b[0m, in \u001b[0;36mParallelFoldFittingStrategy.after_all_folds_scheduled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mray\u001b[39m.\u001b[39mis_initialized():\n\u001b[1;32m    507\u001b[0m     ray_init_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ray_init_args()\n\u001b[0;32m--> 508\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mray\u001b[39m.\u001b[39;49minit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mray_init_args)\n\u001b[1;32m    509\u001b[0m head_node_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mray\u001b[39m.\u001b[39mget_runtime_context()\u001b[39m.\u001b[39mget_node_id()\n\u001b[1;32m    510\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDispatching folds on node \u001b[39m\u001b[39m{\u001b[39;00mhead_node_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/ray/_private/worker.py:1555\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     logger\u001b[39m.\u001b[39minfo(info_str)\n\u001b[0;32m-> 1555\u001b[0m connect(\n\u001b[1;32m   1556\u001b[0m     _global_node,\n\u001b[1;32m   1557\u001b[0m     _global_node\u001b[39m.\u001b[39;49msession_name,\n\u001b[1;32m   1558\u001b[0m     mode\u001b[39m=\u001b[39;49mdriver_mode,\n\u001b[1;32m   1559\u001b[0m     log_to_driver\u001b[39m=\u001b[39;49mlog_to_driver,\n\u001b[1;32m   1560\u001b[0m     worker\u001b[39m=\u001b[39;49mglobal_worker,\n\u001b[1;32m   1561\u001b[0m     driver_object_store_memory\u001b[39m=\u001b[39;49m_driver_object_store_memory,\n\u001b[1;32m   1562\u001b[0m     job_id\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1563\u001b[0m     namespace\u001b[39m=\u001b[39;49mnamespace,\n\u001b[1;32m   1564\u001b[0m     job_config\u001b[39m=\u001b[39;49mjob_config,\n\u001b[1;32m   1565\u001b[0m     entrypoint\u001b[39m=\u001b[39;49mray\u001b[39m.\u001b[39;49m_private\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mget_entrypoint_name(),\n\u001b[1;32m   1566\u001b[0m )\n\u001b[1;32m   1567\u001b[0m \u001b[39mif\u001b[39;00m job_config \u001b[39mand\u001b[39;00m job_config\u001b[39m.\u001b[39mcode_search_path:\n\u001b[1;32m   1568\u001b[0m     global_worker\u001b[39m.\u001b[39mset_load_code_from_local(\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/gluon/lib/python3.10/site-packages/ray/_private/worker.py:2067\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint)\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2066\u001b[0m     logs_dir \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mget_logs_dir_path()\n\u001b[0;32m-> 2067\u001b[0m worker\u001b[39m.\u001b[39mcore_worker \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49m_raylet\u001b[39m.\u001b[39;49mCoreWorker(\n\u001b[1;32m   2068\u001b[0m     mode,\n\u001b[1;32m   2069\u001b[0m     node\u001b[39m.\u001b[39;49mplasma_store_socket_name,\n\u001b[1;32m   2070\u001b[0m     node\u001b[39m.\u001b[39;49mraylet_socket_name,\n\u001b[1;32m   2071\u001b[0m     job_id,\n\u001b[1;32m   2072\u001b[0m     gcs_options,\n\u001b[1;32m   2073\u001b[0m     logs_dir,\n\u001b[1;32m   2074\u001b[0m     node\u001b[39m.\u001b[39;49mnode_ip_address,\n\u001b[1;32m   2075\u001b[0m     node\u001b[39m.\u001b[39;49mnode_manager_port,\n\u001b[1;32m   2076\u001b[0m     node\u001b[39m.\u001b[39;49mraylet_ip_address,\n\u001b[1;32m   2077\u001b[0m     (mode \u001b[39m==\u001b[39;49m LOCAL_MODE),\n\u001b[1;32m   2078\u001b[0m     driver_name,\n\u001b[1;32m   2079\u001b[0m     log_stdout_file_path,\n\u001b[1;32m   2080\u001b[0m     log_stderr_file_path,\n\u001b[1;32m   2081\u001b[0m     serialized_job_config,\n\u001b[1;32m   2082\u001b[0m     node\u001b[39m.\u001b[39;49mmetrics_agent_port,\n\u001b[1;32m   2083\u001b[0m     runtime_env_hash,\n\u001b[1;32m   2084\u001b[0m     startup_token,\n\u001b[1;32m   2085\u001b[0m     session_name,\n\u001b[1;32m   2086\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m mode \u001b[39m!=\u001b[39;49m SCRIPT_MODE \u001b[39melse\u001b[39;49;00m entrypoint,\n\u001b[1;32m   2087\u001b[0m )\n\u001b[1;32m   2089\u001b[0m \u001b[39m# Notify raylet that the core worker is ready.\u001b[39;00m\n\u001b[1;32m   2090\u001b[0m worker\u001b[39m.\u001b[39mcore_worker\u001b[39m.\u001b[39mnotify_raylet()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictor1 = TabularPredictor(label=\"pv_measurement\", eval_metric='mean_absolute_error').fit(\n",
    "    train1,\n",
    "    # time_limit=60*60*1.5,\n",
    "    # hyperparameters='extrme', \n",
    "    presets='best_quality',\n",
    "    # tuning_data = tuning1,\n",
    "    # use_bag_holdout=True,\n",
    "    # num_bag_folds= 6,\n",
    "    # refit_full = True,\n",
    "    # auto_stack = True,\n",
    "    # num_bag_sets= 10,\n",
    "    # set_best_to_refit_full= True,\n",
    "    # num_stack_levels = 2,\n",
    "    # verbosity = 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pipin.get_test_data(\"A\")\n",
    "test_data1 = TabularDataset(test1)\n",
    "\n",
    "\n",
    "pred1 = pd.DataFrame(predictor1.predict(test_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231109_174933/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231109_174933/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   792.58 GB / 994.66 GB (79.7%)\n",
      "Train Data Rows:    25667\n",
      "Train Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, 0.0, 108.0757, 213.23738)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18013.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.13 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 46 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 44 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.1s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.98 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-40.945\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-41.0654\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.4561\t = Validation score   (-mean_absolute_error)\n",
      "\t17.03s\t = Training   runtime\n",
      "\t46.42s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.0126\t = Validation score   (-mean_absolute_error)\n",
      "\t23.53s\t = Training   runtime\n",
      "\t14.72s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-26.7819\t = Validation score   (-mean_absolute_error)\n",
      "\t13.27s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.5788\t = Validation score   (-mean_absolute_error)\n",
      "\t177.98s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-26.4656\t = Validation score   (-mean_absolute_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-27.6384\t = Validation score   (-mean_absolute_error)\n",
      "\t12.17s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.9408\t = Validation score   (-mean_absolute_error)\n",
      "\t81.07s\t = Training   runtime\n",
      "\t38.53s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-25.5741\t = Validation score   (-mean_absolute_error)\n",
      "\t32.99s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.3324\t = Validation score   (-mean_absolute_error)\n",
      "\t72.12s\t = Training   runtime\n",
      "\t119.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-22.0733\t = Validation score   (-mean_absolute_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.9373\t = Validation score   (-mean_absolute_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.516\t = Validation score   (-mean_absolute_error)\n",
      "\t1.41s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-22.205\t = Validation score   (-mean_absolute_error)\n",
      "\t20.88s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.5689\t = Validation score   (-mean_absolute_error)\n",
      "\t8.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-21.9984\t = Validation score   (-mean_absolute_error)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.064\t = Validation score   (-mean_absolute_error)\n",
      "\t12.45s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.3701\t = Validation score   (-mean_absolute_error)\n",
      "\t2.94s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.3534\t = Validation score   (-mean_absolute_error)\n",
      "\t25.05s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.6017\t = Validation score   (-mean_absolute_error)\n",
      "\t4.55s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-21.6598\t = Validation score   (-mean_absolute_error)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 576.52s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231109_174933/\")\n"
     ]
    }
   ],
   "source": [
    "predictor2 = TabularPredictor(label=\"pv_measurement\", eval_metric='mean_absolute_error').fit(\n",
    "    train2,\n",
    "\n",
    "    presets='best_quality', \n",
    "    # hyperparameters='very_large', \n",
    "    # time_limit=60*60*1.5\n",
    "    # tuning_data = tuning2,\n",
    "    # use_bag_holdout=True,\n",
    "    # num_bag_folds= 6,\n",
    "    # refit_full = True,\n",
    "    # auto_stack = True,\n",
    "    # num_bag_sets= 10,\n",
    "    # set_best_to_refit_full= True,\n",
    "    # num_stack_levels = 2,\n",
    "    # verbosity = 3\n",
    "    )\n",
    "\n",
    "# tuning_data\n",
    "# num bag holdout 6\n",
    "# bag_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231109_175910/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231109_175910/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   789.49 GB / 994.66 GB (79.4%)\n",
      "Train Data Rows:    20905\n",
      "Train Data Columns: 46\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 96.74745, 180.17896)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18390.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.18 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 45 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.1s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.97 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-32.3099\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-32.3392\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.9083\t = Validation score   (-mean_absolute_error)\n",
      "\t16.79s\t = Training   runtime\n",
      "\t37.98s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.1397\t = Validation score   (-mean_absolute_error)\n",
      "\t136.22s\t = Training   runtime\n",
      "\t30.44s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-23.528\t = Validation score   (-mean_absolute_error)\n",
      "\t8.37s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.6178\t = Validation score   (-mean_absolute_error)\n",
      "\t2153.25s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-23.3046\t = Validation score   (-mean_absolute_error)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.7934\t = Validation score   (-mean_absolute_error)\n",
      "\t10.16s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.1726\t = Validation score   (-mean_absolute_error)\n",
      "\t991.03s\t = Training   runtime\n",
      "\t21.64s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.1942\t = Validation score   (-mean_absolute_error)\n",
      "\t243.61s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.7619\t = Validation score   (-mean_absolute_error)\n",
      "\t207.02s\t = Training   runtime\n",
      "\t219.53s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-19.615\t = Validation score   (-mean_absolute_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.4903\t = Validation score   (-mean_absolute_error)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.27\t = Validation score   (-mean_absolute_error)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-19.8383\t = Validation score   (-mean_absolute_error)\n",
      "\t1035.81s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.3439\t = Validation score   (-mean_absolute_error)\n",
      "\t7.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-19.7802\t = Validation score   (-mean_absolute_error)\n",
      "\t2.13s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.9973\t = Validation score   (-mean_absolute_error)\n",
      "\t10.31s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.2277\t = Validation score   (-mean_absolute_error)\n",
      "\t3.35s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.2114\t = Validation score   (-mean_absolute_error)\n",
      "\t998.46s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.2636\t = Validation score   (-mean_absolute_error)\n",
      "\t4.35s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-19.5438\t = Validation score   (-mean_absolute_error)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5892.06s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231109_175910/\")\n"
     ]
    }
   ],
   "source": [
    "predictor3 = TabularPredictor(label=\"pv_measurement\", eval_metric='mean_absolute_error' ).fit(\n",
    "    train3,\n",
    "\n",
    "    presets='best_quality', \n",
    "    # time_limit=60*60*1.5\n",
    "    # hyperparameters='very_large', \n",
    "    # tuning_data = tuning3,\n",
    "    # use_bag_holdout=True,\n",
    "    # num_bag_folds= 6,\n",
    "    # refit_full = True,\n",
    "    # auto_stack = True,\n",
    "    # num_bag_sets= 10,\n",
    "    # set_best_to_refit_full= True,\n",
    "    # num_stack_levels = 2,\n",
    "    # verbosity = 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pipin.get_test_data(\"A\")\n",
    "test2 = pipin.get_test_data(\"B\")\n",
    "test3 = pipin.get_test_data(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1 = TabularDataset(test1)\n",
    "test_data2 = TabularDataset(test2)\n",
    "test_data3 = TabularDataset(test3)\n",
    "\n",
    "# pred1 = pd.DataFrame(predictor1.predict(test_data1))\n",
    "pred2 = pd.DataFrame(predictor2.predict(test_data2))\n",
    "pred3 = pd.DataFrame(predictor3.predict(test_data3))\n",
    "\n",
    "negatives_pred1 = pred1[pred1[\"pv_measurement\"] < 0]\n",
    "negatives_pred2 = pred2[pred2[\"pv_measurement\"] < 0]\n",
    "negatives_pred3 = pred3[pred3[\"pv_measurement\"] < 0]\n",
    "neg = pd.concat([negatives_pred1, negatives_pred2, negatives_pred3])\n",
    "neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_unique_filename\n",
    "pred = pd.concat([pred1, pred2, pred3])\n",
    "final_prediction = pipin.post_processing(pred, prediction_column=\"pv_measurement\")\n",
    "file_name = f\"gluon_3_remove_consecutive_measurements_{threshold1}_{threshold2}_{threshold3}\"\n",
    "final_prediction.to_csv(get_unique_filename(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.12465611483225"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = pipin.compare_mae(final_prediction)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.evaluate(df1, silent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
