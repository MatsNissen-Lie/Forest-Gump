{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Master import Master\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/matsalexander/Desktop/Forest Gump/henning mats\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current working directory:\", current_dir)\n",
    "\n",
    "\n",
    "PATH = \"/Users/matsalexander/Desktop/Forest Gump/\"\n",
    "# Estimate\n",
    "X_train_estimated_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + 'A/X_train_estimated.parquet')\n",
    "X_train_estimated_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_train_estimated.parquet\")\n",
    "X_train_estimated_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_train_estimated.parquet\")\n",
    "\n",
    "# Test estimates\n",
    "X_test_estimated_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/X_test_estimated.parquet\")\n",
    "X_test_estimated_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_test_estimated.parquet\")\n",
    "X_test_estimated_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_test_estimated.parquet\")\n",
    "\n",
    "# Observations\n",
    "X_train_observed_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/X_train_observed.parquet\")\n",
    "X_train_observed_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_train_observed.parquet\")\n",
    "X_train_observed_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_train_observed.parquet\")\n",
    "\n",
    "# Targets\n",
    "Y_train_observed_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/train_targets.parquet\")\n",
    "Y_train_observed_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/train_targets.parquet\")\n",
    "Y_train_observed_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/train_targets.parquet\")\n",
    "\n",
    "test_df_example = pd.read_csv(PATH + \"test.csv\")\n",
    "\n",
    "best_submission: pd.DataFrame = pd.read_csv(\n",
    "    PATH + \"mikael/submissions/fourth_submission.csv\")\n",
    "\n",
    "optins = {\n",
    "    \"randomize\": False,\n",
    "    \"consecutive_threshold\": 6,\n",
    "    \"normalize\": False,\n",
    "    \"group_by_hour\": True,\n",
    "    \"unzip_date_feature\": True,\n",
    "}\n",
    "\n",
    "# make a options class with the options as attributes\n",
    "\n",
    "class Options:\n",
    "    randomize = False\n",
    "    consecutive_threshold = 6\n",
    "    normalize = False\n",
    "    group_by_hour = True\n",
    "    unzip_date_feature = True\n",
    "\n",
    "    def __init__(self, randomize=False, consecutive_threshold=6, normalize=False, group_by_hour=True, unzip_date_feature=True) -> None:\n",
    "        self.randomize = randomize\n",
    "        self.consecutive_threshold = consecutive_threshold\n",
    "        self.normalize = normalize\n",
    "        self.group_by_hour = group_by_hour\n",
    "        self.unzip_date_feature = unzip_date_feature\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_combined_data(self, test_data = False):\n",
    "        locations = [\"A\", \"B\", \"C\"]\n",
    "        dfs = []\n",
    "        for index , location in enumerate(locations):\n",
    "            if test_data:\n",
    "                dfs.append(self.get_test_data(location))\n",
    "            else: dfs.append(self.get_data(location))\n",
    "            \n",
    "            dfs[index] = self.onehot_location(dfs[index], location)\n",
    "            df = pd.concat(dfs).reset_index(drop=True)\n",
    "            \n",
    "        if test_data:\n",
    "            return df\n",
    "        return df[[c for c in df if c not in ['pv_measurement']] + #pv measurement is the target and is at the end columns\n",
    "                ['pv_measurement']]\n",
    "    \n",
    "        \n",
    "\n",
    "    def get_data(self, location: str) -> pd.DataFrame:\n",
    "        train, targets = self.get_training_data_by_location(location)\n",
    "        return self.handle_data(train, targets)\n",
    "\n",
    "    def get_test_data(self, location: str) -> pd.DataFrame:\n",
    "        test_data = self.get_test_data_by_location(location)\n",
    "        return self.handle_data(test_data)\n",
    "\n",
    "    def handle_data(self, df, targets = pd.DataFrame()):\n",
    "        df[\"date_calc\"] = pd.to_datetime(df[\"date_calc\"])\n",
    "        df[\"date_forecast\"] = pd.to_datetime(df[\"date_forecast\"])\n",
    "        \n",
    "        df = self.add_time_since_calucation(df)\n",
    "        df = self.onehot_estimated(df)\n",
    "        df = self.unzip_date_feature(df)\n",
    "        df = self.grouped_by_hour(df)\n",
    "        \n",
    "        df[\"time\"] = df[\"date_forecast\"]\n",
    "        df.drop([\"date_forecast\"], axis=1, inplace=True)\n",
    "        if not targets.empty:\n",
    "            df = self.merge_train_target(df, targets)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    # –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– helper funciton ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "    def get_training_data_by_location(self, location):\n",
    "        if location == \"A\":\n",
    "            X_train_observed_x = X_train_observed_a\n",
    "            X_train_estimated_x = X_train_estimated_a\n",
    "            Y_train_x = Y_train_observed_a\n",
    "        elif location == \"B\":\n",
    "            X_train_observed_x = X_train_observed_b\n",
    "            X_train_estimated_x = X_train_estimated_b\n",
    "            Y_train_x = Y_train_observed_b\n",
    "        elif location == \"C\":\n",
    "            X_train_observed_x = X_train_observed_c\n",
    "            X_train_estimated_x = X_train_estimated_c\n",
    "            Y_train_x = Y_train_observed_c\n",
    "        else:\n",
    "            raise Exception(\"location must be A, B or C\")\n",
    "        train = pd.concat(\n",
    "            [X_train_observed_x, X_train_estimated_x]).reset_index(drop=True)\n",
    "        return train, Y_train_x\n",
    "    \n",
    "    def get_test_data_by_location(self, location: str,  normalize=False) -> pd.DataFrame:\n",
    "        if location == \"A\":\n",
    "            df = X_test_estimated_a\n",
    "        elif location == \"B\":\n",
    "            df = X_test_estimated_b\n",
    "        elif location == \"C\":\n",
    "            df = X_test_estimated_c\n",
    "        else:\n",
    "            raise Exception(\"location must be A, B or C\")\n",
    "        return df.copy()\n",
    "    \n",
    "    def unzip_date_feature(self, df: pd.DataFrame, date_column: str = \"date_forecast\"):\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df[\"day_of_year\"] = df[\"date_forecast\"].dt.day_of_year\n",
    "        df[\"hour\"] = df[\"date_forecast\"].dt.hour\n",
    "        df[\"month\"] = df[\"date_forecast\"].dt.month\n",
    "        return df\n",
    "    \n",
    "    def add_time_since_calucation(self, df):\n",
    "        df[\"date_calc\"] = pd.to_datetime(df[\"date_calc\"])\n",
    "        df[\"calculated_ago\"] = (\n",
    "            df[\"date_forecast\"] - df[\"date_calc\"]).dt.total_seconds()\n",
    "        df[\"calculated_ago\"] = df[\"calculated_ago\"].fillna(\n",
    "            0)\n",
    "        return df\n",
    "    \n",
    "    def onehot_estimated(self, df):\n",
    "        df[\"estimated\"] = 0  # Initialize both columns to 0\n",
    "        df[\"observed\"] = 0\n",
    "        estimated_mask = df[\"date_calc\"].notna()\n",
    "        df.loc[estimated_mask, \"estimated\"] = 1\n",
    "        df.loc[~estimated_mask, \"observed\"] = 1\n",
    "        return df\n",
    "\n",
    "    def onehot_location(self, df, location):\n",
    "        if location == \"A\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 1, 0, 0\n",
    "        elif location == \"B\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 0, 1, 0\n",
    "        elif location == \"C\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 0, 0, 1\n",
    "        return df\n",
    "\n",
    "    def grouped_by_hour(self, df: pd.DataFrame, date_column: str = \"date_forecast\"):\n",
    "        df = df.groupby(pd.Grouper(key=date_column, freq=\"1H\")\n",
    "                        ).mean(numeric_only=True)\n",
    "        all_nan_mask = df.isnull().all(axis=1)\n",
    "        df = df[~all_nan_mask]\n",
    "        return df.reset_index()\n",
    "    \n",
    "    def merge_train_target(self, x, y):\n",
    "        print(x.shape, y.shape)\n",
    "        merged = pd.merge(x, y, on=\"time\", how=\"inner\")\n",
    "        mask = merged[\"pv_measurement\"].notna()\n",
    "        merged = merged.loc[mask].reset_index(drop=True)\n",
    "        return merged\n",
    "        \n",
    "    def remove_consecutive_measurments(self, df: pd.DataFrame, consecutive_threshold=6, consecutive_threshold_for_zero=12):\n",
    "        if consecutive_threshold < 2:\n",
    "            return df\n",
    "\n",
    "        column_to_check = 'pv_measurement'\n",
    "        mask = (df[column_to_check] != df[column_to_check].shift(2)).cumsum()\n",
    "\n",
    "        df['consecutive_count'] = df.groupby(\n",
    "            mask).transform('count')[column_to_check]\n",
    "\n",
    "        mask = (df['consecutive_count'] > consecutive_threshold)\n",
    "        mask_zero = (df['consecutive_count'] > consecutive_threshold_for_zero) & (\n",
    "            df[column_to_check] == 0)\n",
    "        df.drop(columns=[\"consecutive_count\"], inplace=True)\n",
    "\n",
    "        df = df.loc[~mask]\n",
    "        df = df.loc[~mask_zero]\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "pipe = Pipeline()\n",
    "df1 = pipe.get_combined_data(test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34062, 52) (34085, 2)\n",
      "(33627, 52) (32848, 2)\n",
      "(33601, 52) (32155, 2)\n"
     ]
    }
   ],
   "source": [
    "A_B_C_all = pipe.get_combined_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_B_C = pipe.get_combined_data(test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231106_145408/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231106_145408/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   717.82 GB / 994.66 GB (72.2%)\n",
      "Train Data Rows:    92951\n",
      "Train Data Columns: 55\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, -0.0, 287.23232, 766.67011)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14110.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 24.17 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) :  1 | ['time']\n",
      "\t\t('float', [])    : 51 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])      :  3 | ['A', 'B', 'C']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 48 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool'])            :  6 | ['snow_density:kgm3', 'estimated', 'observed', 'A', 'B', ...]\n",
      "\t\t('int', ['datetime_as_int']) :  4 | ['time', 'time.year', 'time.day', 'time.dayofweek']\n",
      "\t0.3s = Fit runtime\n",
      "\t55 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 22.87 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.34s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-285.5358\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t5.51s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-338.1481\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t5.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-63.1136\t = Validation score   (-mean_absolute_error)\n",
      "\t291.93s\t = Training   runtime\n",
      "\t186.62s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-67.5515\t = Validation score   (-mean_absolute_error)\n",
      "\t50.58s\t = Training   runtime\n",
      "\t611.98s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-79.3215\t = Validation score   (-mean_absolute_error)\n",
      "\t269.63s\t = Training   runtime\n",
      "\t2.37s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-74.7248\t = Validation score   (-mean_absolute_error)\n",
      "\t2495.87s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-79.5789\t = Validation score   (-mean_absolute_error)\n",
      "\t11.85s\t = Training   runtime\n",
      "\t2.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-88.8412\t = Validation score   (-mean_absolute_error)\n",
      "\t48.24s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-76.6464\t = Validation score   (-mean_absolute_error)\n",
      "\t427.31s\t = Training   runtime\n",
      "\t152.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.6644\t = Validation score   (-mean_absolute_error)\n",
      "\t114.48s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-62.4111\t = Validation score   (-mean_absolute_error)\n",
      "\t83.98s\t = Training   runtime\n",
      "\t330.95s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-60.5003\t = Validation score   (-mean_absolute_error)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-46.4443\t = Validation score   (-mean_absolute_error)\n",
      "\t45.65s\t = Training   runtime\n",
      "\t157.65s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-47.7319\t = Validation score   (-mean_absolute_error)\n",
      "\t50.77s\t = Training   runtime\n",
      "\t119.22s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-49.4658\t = Validation score   (-mean_absolute_error)\n",
      "\t90.47s\t = Training   runtime\n",
      "\t2.41s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.4164\t = Validation score   (-mean_absolute_error)\n",
      "\t415.24s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-52.8229\t = Validation score   (-mean_absolute_error)\n",
      "\t16.99s\t = Training   runtime\n",
      "\t2.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-54.3307\t = Validation score   (-mean_absolute_error)\n",
      "\t47.94s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.7422\t = Validation score   (-mean_absolute_error)\n",
      "\t48.64s\t = Training   runtime\n",
      "\t1.67s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-57.3417\t = Validation score   (-mean_absolute_error)\n",
      "\t48.99s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-46.2701\t = Validation score   (-mean_absolute_error)\n",
      "\t105.21s\t = Training   runtime\n",
      "\t386.56s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-45.1551\t = Validation score   (-mean_absolute_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4981.04s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231106_145408/\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data = TabularDataset(A_B_C_all)\n",
    "\n",
    "label=\"pv_measurement\"\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(label=label,eval_metric='mean_absolute_error').fit(A_B_C_all,presets=\"best_quality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=predictor.predict(test_A_B_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.028516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.047672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18.838985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>119.500908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  prediction\n",
       "0   0    0.005472\n",
       "1   1   -0.028516\n",
       "2   2    0.047672\n",
       "3   3   18.838985\n",
       "4   4  119.500908"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'prediction': predictions\n",
    "})\n",
    "predictions_df.insert(0, 'id', range(0, len(predictions_df)))\n",
    "\n",
    "predictions_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['prediction'] = pd.to_numeric(predictions_df['prediction'], errors='coerce')\n",
    "\n",
    "predictions_df['prediction'] = predictions_df['prediction'].clip(lower=0)\n",
    "\n",
    "predictions_df.to_csv('predictions_new_pipeline.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
